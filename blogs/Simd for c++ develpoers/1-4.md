# SIMD for c++ developers
最近在学习SIMD的指令，但是并没有找到非常好的中文资料。在网络上偶然看到此文章《SIMD for c++ developers》，对Intel SIMD指令进行了整体介绍，工作之便，以作翻译分享。本人对SIMD尚不熟悉，因此翻译进行的比较缓慢，尚有部分内容并不完全理解，因此本文采用了中英文对照的方式，还请多多指正。
## Introduction
...
## Introduction to SIMD
...
## Programming with SIMD
The main disadvantage of the technology is steep learning curve. It took me couple months to become somewhat familiar with the technology, and a year or so before I could say I’m comfortable using it. I had decades of prior experience programming classic C++, and years of prior experience programming GPUs. Didn’t help. CPU SIMD is very different from both of them.

这项技术的主要难度是陡峭的学习曲线。我花了数月时间入门，大约过了一年才敢说可以流畅地使用它。我有着数十年的c++编程经验，多年的GPU编程经验。然而这些都没有用，CPU的SIMD与这些完全不同。

First it takes time to wrap the head around the concept. That all CPUs are actually vector ones, and every time you’re writing `float a = b + c`; you’re wasting 75% to 87.5% of CPU time, because it could have added 4 or 8 numbers with slightly different instruction, spending exactly same time.

首先，我们需要时间去理解相关概念。现在所有的CPU实际上都支持向量操作，每次你编写的`float a = b + c`，你实际浪费了75%-85%的CPU时间。因为你使用一些其他指令在相同的时间同时计算4-8组数字的加法。

Then it takes much more time to get familiar with actual instructions. When I started learning the technology, I can remember few times when I spent hours trying to do something, come up with a solution that barely outperformed scalar version, only to find out later there’s that one special instruction which only takes a couple of them to do what I did in a page of code, improving performance by a factor of magnitude.

然后，我们需要花费更多的时间熟悉指令。在刚开始学习这项技术时，我花费数小时编写的代码发现与标量版本性能相差无几，直到后来使用了一些特殊的指令并仅用了少量代码后，性能得到了数量级的提升，这些事情屡见不鲜。

Another thing, the documentation is not great. I hope I have fixed it to some extent, but still, that official guide is only useful when you already know which intrinsics do you need. Most developers don’t know what modern SIMD is capable of. I’m writing this article hoping to fix it.

另外，文档并不友好。目前官方文档仍然只对熟悉指令的使用者有用，虽然我希望去完善它。但是多数开发者并不知道SIMD的用途。我写这篇文章也是希望可以改善这种情况。

### Data Types
Most of the time you should be processing data in registers. Technically, many of these instructions can operate directly on memory, but it’s rarely a good idea to do so. The ideal pattern for SIMD code, load source data to registers, do as much as you can while it’s in registers, then store the results into memory. The best case is when you don’t have a result, or it’s very small value like bool or a single integer, there’re instructions to copy values from SIMD registers to general purpose ones.

多数情况，你应该使用寄存器处理数据。技术上讲，许多指令可以直接操作内存，但这并不是一个好主意。使用SIMD代码工作的理想模式是：加载数据到寄存器，尽可能在寄存器中处理这些数据，然后存回内存。最理想的情况是：不需要返回结果时，或者返回结果是一个很小的数比如bool或者单个integer时，再使用某些指令把数据从SIMD寄存器复制到通用寄存器。

In C++, the registers are exposed as variables of the 6 fundamental types. See the table on the right. In assembly code there’s no difference between registers of the same size, these types are just for C++ type checking.

在C++中，SIMD寄存器包括6种基本类型的变量。如右表格所示。相同大小的寄存器之间在汇编层面没有任何区别，类型仅是为了应付c++的类型检查。

|              | 16 bytes | 32 bytes |
| ------------ | -------- | -------- |
| 32-bit float | __m128   | __m256   |
| 640bit float | __m128d  | __m256d  |
| Integers     | __m128i  | __m256i  |

The compiler assigns variables to registers automagically, but remember there’re only 16 registers underneath the compiler. Or just 8 if you’re building a 32-bit binary. If you define too many local variables, and write code with many dependencies preventing compiler from reusing registers, the compiler will evict some variables from registers to RAM on the stack. In some edge cases, this may ruin the performance. Profile your software, and review what the compiler did to your C++ code on the performance critical paths.

编译器自动分配变量到寄存器。但是编译器只能使用16个SIMD寄存器，对于编译32位文件则更是只有8个。如果你定义了过多局部变量，并且代码中之间依赖性较高，导致编译器无法重用寄存器，编译器只好将把部分变量从寄存器迁移到RAM上的栈区。对于一些极限情况，这会导致性能损失。测试软件性能时，请检查编译器如何处理关乎性能的那部分代码。

One caveat about these vector types, compilers define them as a structure or union with arrays inside. Technically you can access individual lanes of the vectors by using these arrays inside, the code compiles and works. The performance is not great, though. Compilers usually implement such code as a roundtrip from register to memory, and then back. Don’t treat SIMD vectors that way, use proper shuffle, insert or extract intrinsics instead.

对于这些向量的类型需要注意：编译器将这些向量定义为一个structure或者union，内部则是数组。技术上，你可以通过访问数组的方式获取向量中某一个lane，代码是可以正常编译并工作的，但是会影响性能。因为这通常会造成数据往返于内存和寄存器之间。因此不要用这种方式使用SIMD向量，使用合适的shuffle、insert或者extract的指令更为合适。

### General Purpose Intrinsics
#### Casting Types
Assembly only knows about 2 vector data types, 16-bytes registers and 32-bytes ones. There’re no separate set of 16/32 byte registers, 16-byte ones are lower halves of the corresponding 32-bytes.

汇编只识别两种向量类型：16字节寄存器和32字节寄存器。16字节寄存器和32字节寄存器不是独立的，16字节寄存器是32字节寄存器的低16字节部分。

But they are different in C++, and there’re intrinsics to re-interpret them to different types. In the table below, rows correspond to source types, and columns represent destination type.

但是在C++中，两者是不同的，因此存在指令用来转换两种类型。如下表，行表示原始数据类型，列表示目标数据类型。

|         | __m128                 | __m128d                | __m128i                | __m256              | __m256d                | __m256i                |
| ------- | ---------------------- | ---------------------- | ---------------------- | ------------------- | ---------------------- | ---------------------- |
| __m128  | =                      | _mm_castps_pd          | _mm_castps_si128       | _mm256_castps12     |                        |                        |
| __m128d | _mm_castpd_ps          | =                      | _mm_castpd_si128       |                     | _mm256_castpd128_pd256 |                        |
| __m128i | _mm_castsi128_ps       | _mm_castsi128_pd       | =                      |                     |                        | _mm256_castsi128_si256 |
| __m256  | _mm256_castps256_ps128 |                        |                        | =                   | _mm256_castps_pd       | _mm256_castps_si256    |
| __m256d |                        | _mm256_castpd256_pd128 |                        | _mm256_castpd_ps    | =                      | _mm256_castpd_si256    |
| __m256i |                        |                        | _mm256_castsi256_si128 | _mm256_castsi256_ps | _mm256_castsi256_pd    | =                      |

All these intrinsics compile into no instructions, so they’re practically free performance wise. They don’t change bits in the registers, so 32-bit `1.0f` float becomes `0x3f800000` in 32-bit lanes of the destination integer register. When casting 16-byte values into 32 bytes, the upper half is undefined.

所有这些指令不会编译成任何指令，因此不耗费任何性能。这些指令不改变寄存器中任何bit，因此32bit float `1.0f`将cast为32bit integer `0x3f800000`。当cast 16字节到32字节时，高字节部分是未定义的（undefined）。

#### Converting Types

There’re also instructions to do proper type conversion. They only support signed 32-bit integer lanes. `_mm_cvtepi32_ps` converts 4 integer 32-bit lanes into 32-bit floats, `_mm_cvtepi32_pd` converts first 2 integers into 64-bit floats, `_mm256_cvtpd_epi32` converts 4 doubles into 4 integers, setting higher 4 lanes of the output to zero. When converting from floats to integer, these instructions use rounding mode specified in MXCSR control and status register. See `_MM_SET_ROUNDING_MODE` macro for how to change that mode. That value is a part of thread state, so the OS persists the value across context switches. (That feature may cause hard to find numerical errors when SIMD is used in a thread pool environment, such as OpenMP. For this reason, instead of changing MXCSR I prefer SSE 4.1 rounding intrinsics.)

有一些指令可以执行类型conversion。目前只支持有符号的32bit integer lanes。`_mm_cvtepi32_ps` convert 4个32bit interger lanes至32bit float。`_mm_cvtepi32_pd` convert 前两个integers至64bitfloat，`_mm256_cvtpd_epi32` convert 4个double至interger，剩余的高4个lanes置为0。当从float convert至integer时，round采用MXCSR或status register中指定的模式。通过`_MM_SET_ROUNDING_MODE`宏可以修改round模式。That value is a part of thread state, so the OS persists the value across context switches。（这一特性可能导致在线程池环境下很难发现数值错误，比如OpenMP。因此，相对于更换MXCSR我更喜欢使用SSE4.1指令集中的round指令）

There’re other ones with extra ‘t’ in the name which ignore MXCSR and always use truncation towards zero, like `_mm_cvttpd_epi32` and `_mm_cvttps_epi32`.

使用带有额外“t”字符的指令，可以忽略MXCSR，直接使用向下取整的方式，比如`_mm_cvttpd_epi32`和`_mm_cvttps_epi32`。

There’re also instructions to convert floats between 32- and 64-bit.

有一些指令可以转换32bit与64bit的floats。

#### Memory Access

There’re many ways to load data from memory into registers.

以下指令可以将数据从内存load到寄存器的。

- All types support regular loads, like` _mm_load_si128` or `_mm256_load_ps`. They require source address to be aligned by 16 or 32 bytes. They may crash otherwise but it’s not guaranteed, depends on compiler settings.

- 支持所有类型的常规load，比如`_mm_load_si128`和`_mm256_load_ps`。但是这些指令要求原始数据的地址是16bit或32bit aligned的，否则可能会因为编译器设置造成崩溃。
  
- All types support unaligned loads, like `_mm_loadu_si128` or `_mm256_loadu_ps`. They’re fine with unaligned pointers but aligned loads are faster.

- 所有数据类型支持unaligned load，比如`_mm_loadu_si128`和`_mm256_loadu_ps`。可以正常加载unaligned指针，但是使用aligned loads比unaligned更快。

- `__m128` and `__m128d` types support a few extra. They support single-lane loads, to load just the first lane and set the rest of them to `0.0`, they are `_mm_load_ss` and `_mm_load_sd`. They support intrinsics to load them from memory in reverse order (requires aligned source address).

- `__m128`和`__m128d`支持一些额外的load方式。`_mm_load_ss`和`_mm_load_sd`支持single-lane load，即只load第一个lane，并设置其他lanes为`0.0`。对于aligned的数据，一些指令支持倒序load。

- In AVX, `__m128`, `__m256` and `__m256d` have broadcast load instructions, to load single float or double value from RAM into all lanes of the destination variable. Also `__m256` and `__m256d` types have broadcast loads which read 16 bytes from RAM and duplicate the value in 2 halves of the destination register.

- 在AVX指令集中有处理`__m128`、`__m256`、`__m256d`的broadcast load指令，可以从内存中load一个float或者double到目标的所有lanes中。有些指令可以从内存中读取16字节到`__m256`或`__m256d`，然后复制到寄存器的另一半。

- AVX 1 introduced masked load instructions, they load some lanes and zero out others.

- AVX1指令集中引入了masked load 指令，可以只load一部分lanes，其他的置为0。

- AVX 2 introduced gather load instruction, like `_mm_i32gather_ps`, they take a base pointer, integer register with offsets, and also integer value to scale the offsets. Handy at times, unfortunately rather slow.

- AVX2指令集中加入了gather load指令，如`_mm_i32gather_ps`，可以load原始指针、带偏移的interger、带缩放的interger。方便的同时，比较慢。

- Finally, in many cases you don’t need to do anything special to load values. If your source data is aligned, you can just write code like `__m128i value = *pointer`; and it’ll compile into an equivalent of regular load.

- 最后，在很多情况，你在load时不需要做任何特殊的操作。如果你的数据是aligned，你可以直接写做`__m128i value = *pointer`；编译器会将其编译为常规的加载方式。

Similarly, there’s many ways to store stuff: aligned, unaligned, single-lane, masked in AVX 1.

同样，在AVX1指令集中，针对aligned、unaligned、single-lane、masked有多种方式store。

Couple general notes on RAM access.

1. On modern PCs, RAM delivers at least 8 bytes per load. If you have dual-channel RAM, it delivers 16 bytes. This makes it much faster to access memory in aligned 16- or 32-bytes blocks. If you look at the source code of a modern version of memcpy or memmove library routine, you’ll usually find a manually vectorized version which uses SSE 2 instructions to copy these bytes around. Similarly, when you need something transposed, it’s often much faster to load continuously in blocks of 16 or 32 bytes, then transpose in registers with shuffle instructions.

现代计算机，RAM每次load至少load 8字节。对于双通道内存，每次load 16字节。因此读取aligned的16或者32字节块是很快的。如果你查看memcpy或memmove函数在最新的源代码，你可以看到使用SSE2指令使用向量化指令复制字节。同样当你transpose时，load连续的16或32字节块到寄存器中，然后使用shuffle指令通常更快。

2. Many instructions support memory source for one of the operands. When compiling for SSE-only targets, this requires aligned loads. When compiling for AVX targets, this works for both aligned and unaligned loads. Code like `_mm_add_ps( v, _mm_loadu_ps( ptr ) )` only merges the load when compiling for AVX, while `_mm_add_ps( v, _mm_load_ps( ptr ) )` merges for all targets because the load is aligned (but might crash in runtime if the passed address is not actually amultiple of 16 bytes).

很多指令支持memory source for one of the operands。当只使用SSE指令集编译时，要求aligned load。当使用AVX指令集编译时，aligned或unaligned load均可。`_mm_add_ps( v, _mm_loadu_ps( ptr ) )`只当使用AVX指令集编译时，add才能运行。`_mm_add_ps( v, _mm_load_ps( ptr ) )`对于两个指令集add都可以运行是因为是数据是aligned的（但是如果运行时传入的并不是16字节aligned的，则会发生崩溃）。

3. Most single-lane loads/store can only use lowest lane of destination/source register. If you’re implementing a reduction like sum or average, try to do so you have result in the first lane. The exceptions are `_mm256_insertf128_ps`, `_mm256_insertf128_pd`, `_mm256_inserti128_si256` AVX instructions, they can load 16-byte vectors into higher half of the destination register with little to no performance penalty compared to regular 16-byte load.

多数single-lane的loads或store只支持操作寄存器的最低lane。如果你想要实现reduction比如sum或average，将结果存放在第一个lane。AVX指令 `_mm256_insertf128_ps`, `_mm256_insertf128_pd`, `_mm256_inserti128_si256`例外，这些指令load 16字节的向量到寄存器的高一半部分，而且相对常规的16字节load相比几乎没有性能损失。

4. There’re instructions to load or store bypassing CPU caches, `_mm[256]_stream_something`. Can be useful for cases like video processing, when you know you’re writing stuff that will only be loaded much later, after you’ve processed the complete frame, and probably by another CPU core (L1 and L2 caches are per-core, only L3 is shared).

有一些指令如`_mm[256]_stream_something`可以绕过CPU缓存 load store数据。在视频处理时会有用。when you know you’re writing stuff that will only be loaded much later, after you’ve processed the complete frame, and probably by another CPU core (L1 and L2 caches are per-core, only L3 is shared).

5. Most current CPUs can do twice as many loads per cycle, compared to stores.

多数现代CPU每个cycle load速度是store的两倍。

### Initializing Vector Registers
#### Initializing with Zeros
All the vector types have intrinsics like `_mm_setzero_ps` and `_mm256_setzero_si256` to initialize a register with all zeros. It compiles into code like `xorps` `xmm0`, `xmm0`, `xmm0`. Zero initialization is very efficient because that XOR instruction has no dependencies, the CPU just renames a new register.(The hardware has much more than 16, it’s just 16 names in assembly. The hardware has thousands of them, the pipeline is deep and filled with in-flight instructions who need different versions of same logical registers.)

所有类型向量都有对应的初始化为0的指令，比如`_mm_setzero_ps`和`_mm256_setzero_si256`将寄存器全部初始化为0。代码编写的形式如xorps xmm0, xmm0, xmm0。初始化为0是很快的因为不依赖于XOR指令，CPU只需要rename一个新的寄存器（硬件超过16个寄存器，只不过有16个名称。硬件实际上有上千个，流水线很深，包括很多动态指令，这些指令需要多个版本的相同逻辑的寄存器）。

#### Initializing with Values

CPUs can’t initialize registers with constants apart from 0, but compilers pretend they can, and expose intrinsics like `_mm[256]_set_something` to initialize lanes with different values, and `_mm[256]_set1_something` to initialize all lanes with the same value. If the arguments are compile time constants, they usually compile into read-only data in the compiled binary. Compilers emit the complete 16/32 bytes value, and they make sure it’s aligned. if the arguments aren’t known at compile time, the compilers will do something else reasonable, e.g. if the register is mostly 0 and you only setting one lane they’ll probably emit `_mm_insert_something` instruction. If the values come from variables, the compiler may emit shuffles, or scalar stores followed by vector load.

CPU不能将寄存器初始化除了0以外的常量，但是编译器假装CPU可以。`_mm[256]_set_something`可以将lanes初始化不同的值，`_mm[256]_set1_something`可以初始化全部lanes为一个值。如果变量是编译期常量，该变量将编译为二进制文件的read-only data区域。编译器编译出完整的16/32bytes数值，因此保证数据是aligned。如果变量在编译期是未知的，编译器会做一些合理性动作，比如寄存器只有一个lane不是0，编译器会使用使用`_mm_insert_something` 指令。如果值从变量中拷贝而来，编译器会在load之后使用shuffles、scalar stores等。

For all their `_mm[256]_set_something` intrinsics, Intel screwed up the order. To make a register with integer values `[ 1, 2, 3, 4 ]`, you have to write either `_mm_set_epi32( 4, 3, 2, 1 )` or `_mm_setr_epi32( 1, 2, 3, 4 )`。

对于所有`_mm[256]_set_something`指令，Intel有一些特殊要求。当往寄存器中存入`[ 1, 2, 3, 4 ]`时，你需要使用`_mm_set_epi32( 4, 3, 2, 1 )`或者`_mm_setr_epi32( 1, 2, 3, 4 )`。

### Bitwise Instructions
Both floats and integers have a fairly complete set of bitwise instructions. All of them have AND, OR, XOR, and ANDNOT instructions. If you need bitwise NOT, the fastest way is probably XOR with all ones. Example for 16-byte values:

floats和interger都有几乎完整的bitwise指令，包括AND，OR，XOR和ANDNOT。如果你需要bitwise NOT，最快的方式是与全1做XOR，比如对于16字节的数据：

```c++
__m128i bitwiseNot( __m128i x ) {
const __m128i zero = _mm_setzero_si128();
const __m128i one = _mm_cmpeq_epi32( zero, zero );
return _mm_xor_si128( x, one );
}
```

Whenever you need a register value with all bits set to 1, it’s often a good idea to do what I did here, setzero then compare zeroes with zeroes.

当你需要全1的寄存器，最好的方式如上所示，setzero然后0与0做比较。